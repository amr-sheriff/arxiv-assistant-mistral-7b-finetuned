{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyMP/Is0o6+IboCxiL2RrmK5"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **arXiv Instruct Synthetic Dataset**\n",
    "\n",
    "This Jupyter notebook generates a synthetic instruction-following dataset based on arXiv full-text chunks. It leverages the Together API to process scientific paper segments and create instruction-output pairs for various tasks such as summarization, question answering, and information extraction.\n",
    "\n",
    "The notebook is designed to efficiently process large amounts of text data and can be easily adapted for different models or instruction types. It also demonstrates how to upload the generated dataset to Hugging Face Hub for easy sharing and distribution.\n",
    "\n",
    "The notebook includes the following key components:\n",
    "\n",
    "1. **Setup and Configuration**:\n",
    "   - Installs necessary libraries (together, datasets, tqdm, python-dotenv)\n",
    "   - Imports required modules and sets up API authentication for Hugging Face and Together AI\n",
    "\n",
    "2. **Data Loading**:\n",
    "   - Loads the arXiv full-text chunked dataset from Hugging Face\n",
    "\n",
    "3. **Data Processing**:\n",
    "   - Utilizes multiprocessing for efficient chunk processing\n",
    "   - Applies random selection of instruction templates (summarization, question answering, information extraction)\n",
    "   - Generates outputs using the Together AI API with the `Mixtral-8x22B-Instruct` model\n",
    "\n",
    "4. **Output Generation**:\n",
    "   - Creates a JSONL file containing the synthetic instruction-following dataset\n",
    "\n",
    "5. **Dataset Upload**:\n",
    "   - Converts the JSONL data to a Hugging Face Dataset format\n",
    "   - Uploads the dataset to the Hugging Face Hub\n",
    "\n",
    "The `utils.py` file contains supporting functions:\n",
    "- Instruction templates for different tasks\n",
    "- Functions for interacting with the Together AI API\n",
    "- A process_chunk function that generates a datapoint for each text chunk\n",
    "\n",
    "Author: Amr Achraf  \n",
    "Created Date: 2024-06-13  \n",
    "Updated Date: 2024-09-27  \n",
    "Version: 2.0"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install together\n",
    "!pip install -q datasets\n",
    "!pip install tqdm\n",
    "!pip install python-dotenv"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8l3L28X-u2X4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1717742308641,
     "user_tz": -240,
     "elapsed": 6444,
     "user": {
      "displayName": "Amr Achraf",
      "userId": "01741171265289641694"
     }
    },
    "outputId": "c3eab119-3142-4590-fc37-b12263dc4539",
    "ExecuteTime": {
     "end_time": "2024-06-07T09:13:22.381048Z",
     "start_time": "2024-06-07T09:13:21.105918Z"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login, login\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "hfToken = os.getenv('hf')\n",
    "login(token=hfToken, add_to_git_credential=True, new_session=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsGdqQabyr8I",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1717742410460,
     "user_tz": -240,
     "elapsed": 2180,
     "user": {
      "displayName": "Amr Achraf",
      "userId": "01741171265289641694"
     }
    },
    "outputId": "fe395316-2567-4610-b027-1a4e7604fda0",
    "ExecuteTime": {
     "end_time": "2024-06-07T09:13:28.119122Z",
     "start_time": "2024-06-07T09:13:27.174888Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T09:13:30.374333Z",
     "start_time": "2024-06-07T09:13:30.370450Z"
    }
   },
   "cell_type": "code",
   "source": "togetherToken= os.getenv('togetherAPI')",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = load_dataset(\"amrachraf/arXiv-full-text-chunked\", \"chunk_4\", split=\"train\")"
   ],
   "metadata": {
    "id": "zHPQjPl9zP2K",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1717743013787,
     "user_tz": -240,
     "elapsed": 1301,
     "user": {
      "displayName": "Amr Achraf",
      "userId": "01741171265289641694"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-06-07T09:13:36.358480Z",
     "start_time": "2024-06-07T09:13:31.396015Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DejPEbHn1h6K",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1717743013788,
     "user_tz": -240,
     "elapsed": 2,
     "user": {
      "displayName": "Amr Achraf",
      "userId": "01741171265289641694"
     }
    },
    "outputId": "7988b3f0-7ad5-4811-a079-ac9ad5e0c690",
    "ExecuteTime": {
     "end_time": "2024-06-07T09:13:36.390646Z",
     "start_time": "2024-06-07T09:13:36.367919Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from utils import *\n",
    "from together import Together\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "client = Together(api_key=togetherToken)\n",
    "templates = [summarization_template, question_answering_template, information_extraction_template]\n",
    "\n",
    "total_chunks = len(data)\n",
    "synthetic_data = []\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as executor:\n",
    "    futures = {executor.submit(process_chunk, item, client): item for item in data}\n",
    "\n",
    "    with tqdm(total=total_chunks, desc=\"Processing chunks\", unit=\"chunk\") as pbar:\n",
    "      for future in as_completed(futures):\n",
    "        result = future.result(timeout=30)\n",
    "        synthetic_data.append(result)\n",
    "        pbar.update(1)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "ggPH50SkvyB9",
    "executionInfo": {
     "status": "error",
     "timestamp": 1717746539288,
     "user_tz": -240,
     "elapsed": 60985,
     "user": {
      "displayName": "Amr Achraf",
      "userId": "01741171265289641694"
     }
    },
    "outputId": "bea10bf1-79f6-4499-b82a-b30e9cdb7d0e",
    "ExecuteTime": {
     "end_time": "2024-06-07T15:34:40.621784Z",
     "start_time": "2024-06-07T09:28:11.605652Z"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open(\"synthetic_finetuning_data.jsonl\", \"w\") as f:\n",
    "    for item in synthetic_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3QK9Jsubuke",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1717746063614,
     "user_tz": -240,
     "elapsed": 6,
     "user": {
      "displayName": "Amr Achraf",
      "userId": "01741171265289641694"
     }
    },
    "outputId": "a3a5520c-f726-4c74-d1a5-4e5011880990",
    "ExecuteTime": {
     "end_time": "2024-06-07T18:27:01.823908Z",
     "start_time": "2024-06-07T18:27:01.653011Z"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T02:23:46.502401Z",
     "start_time": "2024-06-10T02:23:46.496102Z"
    }
   },
   "cell_type": "code",
   "source": "print(synthetic_data[5]['instruction'])",
   "execution_count": 73,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:45:31.939636Z",
     "start_time": "2024-06-07T19:45:31.826844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_file = \"synthetic_finetuning_data.jsonl\"\n",
    "hf_username = \"amrachraf\"\n",
    "hf_dataset_name = \"arXiv-full-text-synthetic-instruct-tune\"\n",
    "\n",
    "with open(data_file, \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:45:34.756788Z",
     "start_time": "2024-06-07T19:45:34.747572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_jsonl = {\"instruction\": [d[\"instruction\"] for d in data],\n",
    " \"input\": [d[\"input\"] for d in data],\n",
    " \"output\": [d[\"output\"] for d in data]}"
   ],
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:46:28.286177Z",
     "start_time": "2024-06-07T19:46:28.277235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import HfApi, HfFolder, Repository\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "HfFolder.save_token(hfToken)"
   ],
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T19:46:38.340734Z",
     "start_time": "2024-06-07T19:46:30.414824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "try:\n",
    "    dataset = Dataset.from_dict(data_jsonl)\n",
    "    dataset_dict = DatasetDict({\"train\": dataset})\n",
    "\n",
    "    repo_name = f'{hf_username}/{hf_dataset_name}'\n",
    "    hf_api = HfApi()\n",
    "\n",
    "    hf_api.create_repo(repo_name, repo_type=\"dataset\", exist_ok=True)\n",
    "\n",
    "    dataset_dict.push_to_hub(repo_name, private=False, token=hfToken)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to upload dataset {hf_dataset_name}: {e}\")"
   ],
   "execution_count": 45,
   "outputs": []
  }
 ]
}
